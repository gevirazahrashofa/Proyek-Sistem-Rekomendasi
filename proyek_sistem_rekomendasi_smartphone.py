# -*- coding: utf-8 -*-
"""Proyek Sistem Rekomendasi Smartphone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JBuMtLJ1gP2JGWSkwhAne4Qn0oEmdirm

# Sistem Rekomendasi Smartphone

Nama : Gevira Zahra Shofa

## Import Library

Langkah pertama adalah mengimpor library yang diperlukan untuk menjalankan program.
"""

# Library utama untuk manipulasi data dan visualisasi
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

# Untuk pemrosesan teks dan perhitungan kemiripan
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Library untuk membangun model berbasis neural network
import tensorflow as tf
from tensorflow.keras import layers, models

# Mengatur path file
from pathlib import Path

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings('ignore')

"""Selanjutnya, ekstrak file dataset yang telah dikompresi dalam format ZIP."""

import zipfile

with zipfile.ZipFile('cellphones.zip', 'r') as zip_ref:
    zip_ref.extractall('cellphone')

"""Kemudian, ubah nama file agar lebih sederhana dan mudah digunakan dalam proses analisis."""

import os

os.rename('cellphone/cellphones data.csv', 'cellphone/data.csv')
os.rename('cellphone/cellphones ratings.csv', 'cellphone/rating.csv')
os.rename('cellphone/cellphones users.csv', 'cellphone/user.csv')

"""Setelah itu, baca masing-masing file CSV ke dalam bentuk DataFrame menggunakan pandas."""

# Membaca dataset
data_df = pd.read_csv('cellphone/data.csv')
rating_df = pd.read_csv('cellphone/rating.csv')
user_df = pd.read_csv('cellphone/user.csv')

# Menampilkan info dari dataset data.csv
data_df.info()

"""Dataset data berisi 33 data ponsel dengan 14 atribut, mencakup spesifikasi teknis seperti RAM, kamera, dan ukuran baterai, serta informasi tambahan seperti harga dan tahun rilis, tanpa ada nilai yang hilang."""

# Menampilkan isi dari variabel data.csv
data_df.head()

"""Dataset memuat informasi spesifikasi dan harga berbagai model ponsel, dengan variabel seperti merek, memori, kamera, ukuran layar, baterai, dan tanggal rilis yang dapat digunakan untuk analisis performa dan segmentasi pasar."""

data_df.describe()

"""Statistik deskriptif menunjukkan variasi besar dalam spesifikasi dan harga ponsel, dengan harga tertinggi mencapai hampir \$2000 dan kapasitas memori hingga 512GB, mencerminkan rentang produk dari kelas bawah hingga flagship."""

# Menampilkan info dari dataset rating.csv
rating_df.info()

"""Dataset rating berisi 990 data penilaian pengguna terhadap ponsel, dengan tiga kolom utama: user_id, cellphone_id, dan rating, semuanya bertipe numerik dan tidak ada nilai yang hilang."""

# Menampilkan isi dari variabel rating
rating_df.head()

"""Setiap baris dalam dataset rating merepresentasikan penilaian dari seorang pengguna terhadap satu ponsel, dengan skor rating yang bervariasi dari rendah hingga tinggi."""

rating_df.describe()

"""Dataset rating menunjukkan distribusi rating mayoritas berkisar di tengah-tengah (sekitar 5-9) dengan variasi rating cukup lebar dari 1 hingga 18, dan pengguna serta produk yang beragam."""

# Menampilkan info dari dataset user.csv
user_df.info()

"""Dataset user.csv berisi 99 pengguna dengan informasi umur dan gender lengkap, namun terdapat 1 nilai kosong pada kolom occupation."""

# Menampilkan isi dari variabel user.csv
user_df.head()

"""Dataset user menunjukkan pengguna dengan rentang usia beragam, didominasi gender perempuan, dan berbagai jenis pekerjaan di bidang IT dan manajemen."""

user_df.describe()

"""Dataset user memiliki pengguna dengan usia rata-rata sekitar 36 tahun, rentang usia dari 21 hingga 61 tahun, dan distribusi user_id yang beragam dari 0 sampai 258.

## Univariate Exploratory Data Analysis

Lakukan analisis univariate untuk memahami distribusi dan karakteristik dasar masing-masing variabel dalam dataset.

### EDA data
"""

# Tampilkan sampel data pertama
display(data_df.head(1))

"""Data sampel menunjukkan produk smartphone Apple iPhone SE (2022) dengan spesifikasi RAM 4GB, kamera utama 12MP, baterai 2018mAh, dan harga 429, mencerminkan produk kelas menengah ke atas."""

# Hitung variasi brand yang tersedia
unique_brands = data_df.brand.unique()
print(f'Total brand yang tersedia: {len(unique_brands)}')

"""Terdapat total 10 merek smartphone berbeda dalam dataset, menunjukkan variasi produk yang cukup beragam."""

# Distribusi smartphone per brand
brand_distribution = data_df['brand'].value_counts()
display(brand_distribution)

"""Distribusi smartphone didominasi oleh Samsung dengan 8 model, diikuti Apple dengan 6, sementara merek lain memiliki jumlah model yang lebih sedikit dan bervariasi."""

# Visualisasi distribusi brand
fig, ax = plt.subplots(figsize=(5, 3))
data_df['brand'].value_counts().plot(kind='bar', ax=ax)
ax.tick_params(axis='x', rotation=90)
plt.tight_layout()
plt.show()

# Identifikasi semua model yang ada
available_models = data_df['model'].unique()
print(f'Total model smartphone: {len(available_models)}')
display(available_models)

"""Dataset mencakup 33 model smartphone berbeda dari berbagai merek, menunjukkan variasi produk yang cukup luas dan beragam."""

# Distribusi sistem operasi
os_distribution = data_df['operating system'].value_counts()
display(os_distribution)

"""Distribusi sistem operasi didominasi oleh Android dengan 27 model, sementara iOS hanya memiliki 6 model dalam dataset."""

# Grafik distribusi OS
fig, ax = plt.subplots(figsize=(5, 3))
data_df['operating system'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

# Distribusi kapasitas memori internal
memory_distribution = data_df['internal memory'].value_counts()
display(memory_distribution)

"""Kapasitas memori internal yang paling umum adalah 128GB dengan 20 model, diikuti oleh 256GB, 64GB, 32GB, dan hanya satu model dengan 512GB."""

# Visualisasi memori internal
fig, ax = plt.subplots(figsize=(5, 3))
data_df['internal memory'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

# Distribusi kapasitas RAM
ram_distribution = data_df['RAM'].value_counts()
display(ram_distribution)

"""Kapasitas RAM paling banyak tersedia adalah 8GB dengan 13 model, diikuti oleh 4GB dan 6GB masing-masing 6 model, serta 3GB dan 12GB yang lebih sedikit."""

# Grafik distribusi RAM
fig, ax = plt.subplots(figsize=(5, 3))
data_df['RAM'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

# Kategorisasi berdasarkan skor performa
high_performance = data_df.query('performance > 5')
low_performance = data_df.query('performance <= 5')

# Hitung jumlah untuk setiap kategori
count_high = high_performance.shape[0]
count_low = low_performance.shape[0]

print(f'Smartphone dengan performa tinggi (>5): {count_high}')
print(f'Smartphone dengan performa rendah (<=5): {count_low}')

"""Terdapat 23 smartphone dengan performa tinggi (skor > 5) dan 10 smartphone dengan performa rendah (skor â‰¤ 5) dalam dataset."""

# Buat DataFrame untuk visualisasi
perf_summary = pd.DataFrame({
    'Kategori': ['Performa Tinggi (>5)', 'Performa Rendah (<=5)'],
    'Jumlah': [count_high, count_low]
})

# Plot kategorisasi performa
fig, ax = plt.subplots(figsize=(8, 5))
perf_summary.plot(x='Kategori', y='Jumlah', kind='bar', ax=ax, legend=False)
plt.tight_layout()
plt.show()

"""#### Analisis Kamera Utama"""

# Distribusi resolusi kamera utama
camera_distribution = data_df['main camera'].value_counts()
display(camera_distribution)

"""Resolusi kamera utama yang paling umum adalah 50MP dengan 13 model, diikuti 12MP dengan 10 model, serta beberapa model dengan resolusi lebih tinggi seperti 64MP, 48MP, dan 108MP."""

# Visualisasi kamera utama
fig, ax = plt.subplots(figsize=(5, 3))
data_df['main camera'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""#### Analisis Kapasitas Baterai"""

# Distribusi kapasitas baterai
battery_distribution = data_df['battery size'].value_counts()
display(battery_distribution)

"""Kapasitas baterai 5000mAh paling umum dengan 11 model, sementara kapasitas lainnya bervariasi dari 2018mAh hingga 5003mAh dengan jumlah model lebih sedikit."""

# Grafik kapasitas baterai
fig, ax = plt.subplots(figsize=(15, 3))
data_df['battery size'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""#### Analisis Ukuran Layar"""

# Distribusi ukuran layar
screen_distribution = data_df['screen size'].value_counts()
display(screen_distribution)

"""Ukuran layar 6.7 inci paling umum dengan 8 model, diikuti ukuran 6.5 inci dan 6.1 inci, serta variasi ukuran layar lain yang lebih kecil jumlahnya."""

# Visualisasi ukuran layar
fig, ax = plt.subplots(figsize=(5, 3))
data_df['screen size'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""#### Analisis Berat Smartphone"""

# Distribusi berat smartphone
weight_distribution = data_df['weight'].value_counts()
print(f'Variasi berat: {len(weight_distribution)}')
display(weight_distribution)

"""Berat smartphone bervariasi dalam 27 kategori berbeda, dengan berat 204 gram paling sering muncul sebanyak 5 model."""

# Grafik distribusi berat
fig, ax = plt.subplots(figsize=(15, 3))
data_df['weight'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""#### Analisis Tahun Rilis"""

# Duplikasi dataset untuk manipulasi
dataset_copy = data_df.copy()

# Konversi tanggal rilis ke format datetime
dataset_copy['release date'] = pd.to_datetime(dataset_copy['release date'], format='%d/%m/%Y')

# Ekstrak tahun dari tanggal rilis
dataset_copy['year_released'] = dataset_copy['release date'].dt.year

# Hitung distribusi per tahun rilis
yearly_releases = dataset_copy['year_released'].value_counts().sort_index()
display(yearly_releases)

"""Smartphone dalam dataset dirilis terutama pada tahun 2021 dan 2022 dengan jumlah yang sama, sementara hanya satu model yang dirilis pada 2018."""

# Visualisasi rilis per tahun
fig, ax = plt.subplots(figsize=(5, 3))
dataset_copy['year_released'].value_counts().sort_index().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""### EDA Rating

#### Aktivitas Penguna
"""

# Hitung frekuensi review per user
user_activity = rating_df['user_id'].value_counts()
display(user_activity)

# Cek keunikan nilai review per user
print(f"\nVariasi jumlah review per user: {user_activity.unique()}")

"""Setiap user dalam dataset rating memberikan tepat 10 ulasan, menunjukkan distribusi aktivitas review yang seragam antar pengguna.

#### Popularitas smartphone
"""

# Hitung frekuensi review per smartphone
phone_popularity = rating_df['cellphone_id'].value_counts()
print(f"\nDistribusi review per smartphone ID:")
display(phone_popularity.sort_index())

"""Jumlah review per smartphone bervariasi, dengan beberapa model mendapatkan hingga 41 review, menunjukkan tingkat popularitas yang berbeda antar smartphone dalam dataset."""

# Statistik dasar popularitas
print(f"Review minimum per smartphone: {phone_popularity.min()}")
print(f"Review maksimum per smartphone: {phone_popularity.max()}")

"""Jumlah review per smartphone berkisar antara 20 hingga 41, menunjukkan variasi popularitas antar model."""

# Grafik popularitas smartphone
fig, ax = plt.subplots(figsize=(15, 3))
rating_df['cellphone_id'].value_counts().sort_index().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""#### Distribusi Rating"""

# Hitung distribusi nilai rating
rating_distribution = rating_df['rating'].value_counts()
print(f"\nDistribusi nilai rating:")
display(rating_distribution.sort_index())

"""Distribusi rating didominasi oleh nilai 7 dan 8, sementara rating rendah (1-5) juga cukup banyak, dan terdapat satu nilai outlier rating 18."""

# Visualisasi distribusi rating
fig, ax = plt.subplots(figsize=(5, 3))
rating_df['rating'].value_counts().sort_index().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""Sebagian besar pengguna memberikan rating tinggi (7â€“10), dengan rating 8 sebagai yang paling sering muncul.

### EDA User

#### Distribusi Usia
"""

# Analisis distribusi usia pengguna
age_distribution = user_df['age'].value_counts()
print(f"Distribusi usia pengguna:")
display(age_distribution.sort_index())

"""Distribusi usia pengguna beragam, dengan konsentrasi tertinggi pada usia 25 tahun dan rentang usia pengguna dari 21 hingga 61 tahun."""

# Grafik distribusi usia
fig, ax = plt.subplots(figsize=(10, 3))
user_df['age'].value_counts().sort_index().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""Sebagian besar data berusia antara 25 hingga 32 tahun, dengan usia 25 sebagai yang paling dominan.

#### Distribusi Gender
"""

# Analisis distribusi gender pengguna
gender_distribution = user_df['gender'].value_counts()
print(f"Distribusi gender pengguna:")
display(gender_distribution.sort_index())

"""Mayoritas pengguna adalah laki-laki (50) dan perempuan (46), dengan (3) data yang belum terisi pada kategori gender."""

# Visualisasi gender
fig, ax = plt.subplots(figsize=(5, 3))
user_df['gender'].value_counts().plot(kind='bar', ax=ax)
plt.tight_layout()
plt.show()

"""Mayoritas data berjenis kelamin Male dan Female, dengan sedikit entri yang belum memilih gender.

#### Distribusi Pekerjaan
"""

# Analisis distribusi pekerjaan (case-insensitive)
occupation_distribution = user_df['occupation'].str.lower().value_counts()
print(f'Variasi pekerjaan: {len(occupation_distribution)}')
print(f"Distribusi pekerjaan pengguna:")
display(occupation_distribution.sort_index())

"""Pengguna memiliki beragam pekerjaan dengan 45 variasi berbeda, paling banyak berprofesi sebagai manager, information technology, dan IT.

## Preparation Data

### Penggabungan Daatasett
"""

# Tahap 1: Merge dataset rating dengan data cellphone
combined_ratings = rating_df.merge(data_df, on='cellphone_id', how='inner')

# Tahap 2: Merge hasil dengan dataset users
final_dataset = combined_ratings.merge(user_df, on='user_id', how='inner')

# Tampilkan preview dataset gabungan
display(final_dataset.head())

"""Dataset gabungan menghubungkan data rating, smartphone, dan pengguna, memberikan informasi lengkap mulai dari spesifikasi ponsel hingga profil user dan rating yang diberikan.

### Identifikasi missing value
"""

# Periksa keberadaan nilai yang hilang
null_summary = final_dataset.isnull().sum()
display(null_summary)

"""Dalam dataset gabungan, semua kolom lengkap kecuali kolom occupation yang masih memiliki 10 missing value.

### Inspeksi Baris dengan Nilai Null
"""

# Identifikasi dan tampilkan baris yang mengandung nilai null
null_rows = final_dataset[final_dataset.isnull().any(axis=1)]
display(null_rows)

"""Terdapat 10 baris dengan nilai null pada kolom occupation, semua berasal dari user\_id 53 yang juga memiliki gender "-Select Gender-" sebagai nilai tidak terisi.

### Pembersihan Missing Value
"""

# Eliminasi baris dengan nilai yang hilang
final_dataset.dropna(inplace=True)

# Konfirmasi tidak ada lagi missing values
post_clean_nulls = final_dataset.isnull().sum()
display(post_clean_nulls)

"""Setelah penanganan, tidak ada lagi nilai yang hilang di dataset gabungan, semua kolom kini lengkap dan siap untuk analisis lanjut.

### Pembersihan dan Standarisasi Data
"""

# Eliminasi rating anomali (nilai 18)
final_dataset = final_dataset.loc[final_dataset['rating'] != 18]

# Standardisasi format occupation ke lowercase
final_dataset.loc[:, 'occupation'] = final_dataset['occupation'].str.lower()

# Koreksi typo 'healthare' menjadi 'healthcare'
final_dataset.loc[:, 'occupation'] = final_dataset['occupation'].str.replace('healthare', 'healthcare')

# Ekspansi singkatan 'it' menjadi 'information technology'
final_dataset.loc[:, 'occupation'] = final_dataset['occupation'].str.replace('it', 'information technology')

"""Langkah pembersihan sudah diterapkan: rating anomali 18 dihapus, occupation distandarisasi ke lowercase, typo 'healthare' diperbaiki menjadi 'healthcare', dan singkatan 'it' dikembangkan menjadi 'information technology'.

### Finalisasi Dataset Bersih
"""

# Assign dataset yang telah dibersihkan
processed_data = final_dataset.copy()
display(processed_data)

"""## Content-Based Filtering

### Proses duplikasi
"""

# Hapus duplikat berdasarkan cellphone_id
processed_data.drop_duplicates(subset=['cellphone_id'], inplace=True)

"""### Ekstraksi fitur utama"""

# Konversi kolom-kolom penting menjadi struktur list
id_list = processed_data['cellphone_id'].values.tolist()
brand_list = processed_data['brand'].values.tolist()
model_list = processed_data['model'].values.tolist()
os_list = processed_data['operating system'].values.tolist()

# Verifikasi konsistensi panjang data
print(f"Jumlah cellphone_id: {len(id_list)}")
print(f"Jumlah brand: {len(brand_list)}")
print(f"Jumlah model: {len(model_list)}")
print(f"Jumlah operating system: {len(os_list)}")

"""Data sudah dikonversi ke list untuk kolom `cellphone_id`, `brand`, `model`, dan `operating system` dengan jumlah elemen yang konsisten yaitu 33 untuk setiap list. Ada lagi yang ingin kamu lakukan?

### Kontruksi dataset final
"""

# Pembentukan DataFrame baru dengan fitur-fitur utama
phone_dataset = pd.DataFrame({
    'cellphone_id': id_list,
    'brand': brand_list,
    'model': model_list,
    'operating_system': os_list,
})

# Tampilkan dataset final
display(phone_dataset)

"""DataFrame phone_dataset yang berisi kolom cellphone_id, brand, model, dan operating_system sudah terbentuk dan tampilannya sudah benar."""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Initialize variables
tfidf_vectorizer = TfidfVectorizer()
cosine_sim_matrix = None
# phone_data = None # Remove or comment out this line as phone_data is assigned above
indices = None

# Assuming phone_dataset is your DataFrame
# Now phone_dataset contains the DataFrame created in the previous cell
content_data = phone_dataset.copy()

# Normalisasi fitur numerik
numeric_cols = ['performance', 'main camera', 'battery size', 'screen size', 'weight']
for col in numeric_cols:
    # Use content_data instead of phone_data
    if col in content_data.columns:
        min_val = content_data[col].min()
        max_val = content_data[col].max()
        # Ensure the column has more than one unique value to avoid division by zero
        if max_val > min_val:
             content_data[f'{col}_norm'] = (content_data[col] - min_val) / (max_val - min_val)
        else:
             # If all values are the same, set the normalized value to 0 or 1 (or handle as needed)
             content_data[f'{col}_norm'] = 0.5 # Or np.nan if preferred to indicate no variation

# Gabungkan fitur kategorikal dan numerik
categorical_features = ['brand', 'operating_system']

# Filter out features that might have resulted in NaN from normalization if handled that way
# Use content_data here, which is now assigned the DataFrame
numerical_features = [f'{col}_norm' for col in numeric_cols if f'{col}_norm' in content_data.columns and not content_data[f'{col}_norm'].isnull().all()]

content_data['content_profile'] = ''

for feature in categorical_features:
    if feature in content_data.columns:
        content_data['content_profile'] += ' ' + content_data[feature].astype(str)

for feature in numerical_features:
    # Ensure the numerical feature column exists in content_data before accessing it
    if feature in content_data.columns:
        # Handle potential issues with qcut if the data distribution is problematic
        try:
            content_data[feature + '_cat'] = pd.qcut(content_data[feature],
                                                q=4, labels=['low', 'medium', 'high', 'very_high'], duplicates='drop') # Use duplicates='drop' to handle cases with fewer than q unique values
            content_data['content_profile'] += ' ' + content_data[feature + '_cat'].astype(str)
        except ValueError as e:
            print(f"Could not discretize feature {feature}: {e}")
            # Optionally handle or log this error

# Build similarity matrix
# Use content_data instead of phone_data
tfidf_matrix = tfidf_vectorizer.fit_transform(content_data['content_profile'].fillna('')) # Fill NaN in content_profile if any somehow crept in
cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Use content_data instead of phone_data for indices as well
indices = pd.Series(content_data.index, index=content_data['cellphone_id']).drop_duplicates()

print("âœ“ Content-based model berhasil dilatih")
print(f"  - Jumlah smartphone: {len(content_data)}") # Use content_data for count as well
print(f"  - Dimensi similarity matrix: {cosine_sim_matrix.shape}")

"""Content-based similarity matrix sudah terbentuk dengan 33 smartphone dan matriks similarity berdimensi (33, 33) yang sesuai."""

# Get recommendations function
# Define a class for the Content-Based Recommender
class ContentBasedRecommender:
    def __init__(self, indices, cosine_sim_matrix, phone_data):
        self.indices = indices
        self.cosine_sim_matrix = cosine_sim_matrix
        self.phone_data = phone_data # Store the original phone_data or content_data

    def get_recommendations(self, phone_id, top_n=10):
        """Mendapatkan rekomendasi berdasarkan smartphone yang dipilih"""
        if phone_id not in self.indices:
            print(f"Phone ID {phone_id} not found in the dataset.")
            return pd.DataFrame() # Return an empty DataFrame if phone_id is not found

        idx = self.indices[phone_id]
        sim_scores = list(enumerate(self.cosine_sim_matrix[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:top_n+1] # Exclude the item itself (index 0)

        phone_indices = [i[0] for i in sim_scores]
        similarity_scores = [i[1] for i in sim_scores]

        # Ensure we don't try to access indices that are out of bounds if something went wrong with indexing
        # Use the stored phone_data/content_data for indexing
        valid_phone_indices = [i for i in phone_indices if i < len(self.phone_data)]

        recommendations = self.phone_data.iloc[valid_phone_indices][['cellphone_id', 'brand', 'model', 'operating_system']].copy()
        # Match similarity scores to valid indices, keeping the order
        # Need to map back the similarity scores to the valid indices
        recommendation_similarity_scores = [sim_scores[phone_indices.index(i)][1] for i in valid_phone_indices]
        recommendations['similarity_score'] = recommendation_similarity_scores

        return recommendations

# --- Instantiate the ContentBasedRecommender ---
# Make sure 'indices', 'cosine_sim_matrix', and 'content_data' are defined from previous steps
cb_recommender = ContentBasedRecommender(indices, cosine_sim_matrix, content_data)

print("REKOMENDASI CONTENT-BASED:")

sample_phone_id = content_data['cellphone_id'].iloc[0]
sample_phone_info = content_data[content_data['cellphone_id'] == sample_phone_id].iloc[0]

print(f"\nSmartphone yang dipilih:")
print(f"ID: {sample_phone_info['cellphone_id']}")
print(f"Brand: {sample_phone_info['brand']}")
print(f"Model: {sample_phone_info['model']}")
print(f"OS: {sample_phone_info['operating_system']}")

cb_recommendations = cb_recommender.get_recommendations(sample_phone_id, top_n=5)
print(f"\nTop-5 Rekomendasi Content-Based:")
for idx, row in cb_recommendations.iterrows():
    print(f"{idx+1}. {row['brand']} {row['model']} (OS: {row['operating_system']}) - Score: {row['similarity_score']:.3f}")

"""Rekomendasi menunjukkan bahwa smartphone dengan fitur dan merek serupa (Motorola Moto G series) memiliki kesamaan konten sangat tinggi, sementara perangkat dari merek lain seperti Samsung meskipun masih Android, memiliki kesamaan yang jauh lebih rendah.

### Collaborative Filtering
"""

# Buat matrix rating user-item
rating_matrix = final_dataset.pivot_table(
    index='user_id',
    columns='cellphone_id',
    values='rating'
).fillna(0)

print(f"Ukuran rating matrix: {rating_matrix.shape}")
print(f"Jumlah user: {rating_matrix.shape[0]}")
print(f"Jumlah item: {rating_matrix.shape[1]}")

# Tampilkan sample rating matrix
display(rating_matrix.head())

# Konversi rating matrix ke format yang sesuai untuk neural network
def prepare_data_for_nn(rating_df):
    """Menyiapkan data untuk neural network collaborative filtering"""
    # Encode user_id dan cellphone_id
    user_ids = rating_df['user_id'].unique()
    phone_ids = rating_df['cellphone_id'].unique()

    user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}
    phone_to_idx = {phone_id: idx for idx, phone_id in enumerate(phone_ids)}

    # Mapping balik untuk evaluasi
    idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}
    idx_to_phone = {idx: phone_id for phone_id, idx in phone_to_idx.items()}

    # Konversi ke encoded format
    users = rating_df['user_id'].map(user_to_idx).values
    phones = rating_df['cellphone_id'].map(phone_to_idx).values
    ratings = rating_df['rating'].values

    return users, phones, ratings, user_to_idx, phone_to_idx, idx_to_user, idx_to_phone

# Prepare data
users, phones, ratings, user_to_idx, phone_to_idx, idx_to_user, idx_to_phone = prepare_data_for_nn(final_dataset)

print(f"Jumlah unique users: {len(user_to_idx)}")
print(f"Jumlah unique phones: {len(phone_to_idx)}")
print(f"Total ratings: {len(ratings)}")

# Pembagian data train-test
from sklearn.model_selection import train_test_split

# Split data menjadi train dan test (80:20)
train_users, test_users, train_phones, test_phones, train_ratings, test_ratings = train_test_split(
    users, phones, ratings,
    test_size=0.2,
    random_state=42,
    stratify=None  # Tidak stratify karena continuous ratings
)

print(f"Training data: {len(train_ratings)} samples")
print(f"Testing data: {len(test_ratings)} samples")
print(f"Train/Test ratio: {len(train_ratings)/(len(train_ratings)+len(test_ratings)):.2f}/{len(test_ratings)/(len(train_ratings)+len(test_ratings)):.2f}")

"""Data telah dibagi menjadi training set (80%) dan testing set (20%) untuk evaluasi model yang objektif."""

"""Dataset ini berisi 98 pengguna dan 33 produk ponsel dengan total 979 rating, yang menunjukkan data cukup sparse karena tidak semua user memberi rating ke semua item. Data dibagi menjadi 80% training (783 sampel) dan 20% testing (196 sampel) untuk evaluasi model. Variasi rating antar pengguna menunjukkan preferensi yang berbeda-beda, sehingga penting bagi sistem rekomendasi untuk menangani ketidakseimbangan ini agar hasilnya akurat dan relevan.

## Modeling

### Modeling Content-Based Filtering
"""

# Evaluasi Content-Based Filtering
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

class ContentBasedEvaluator:
    def __init__(self, recommender, rating_data, threshold=7):
        self.recommender = recommender
        self.rating_data = rating_data
        self.threshold = threshold  # Rating threshold untuk "liked" items

    def evaluate_precision_recall(self, test_users, top_k=5):
        """Evaluasi precision dan recall untuk content-based filtering"""
        precisions = []
        recalls = []

        for user_id in test_users[:10]:  # Ambil sample user untuk evaluasi
            # Get user's highly rated items
            user_ratings = self.rating_data[self.rating_data['user_id'] == user_id]
            liked_items = user_ratings[user_ratings['rating'] >= self.threshold]['cellphone_id'].tolist()

            if not liked_items:
                continue

            # Get recommendations based on one of user's liked items
            sample_item = liked_items[0]
            recommendations = self.recommender.get_recommendations(sample_item, top_n=top_k)

            if recommendations.empty:
                continue

            recommended_items = recommendations['cellphone_id'].tolist()

            # Calculate precision and recall
            relevant_recommendations = set(recommended_items) & set(liked_items)

            precision = len(relevant_recommendations) / len(recommended_items) if recommended_items else 0
            recall = len(relevant_recommendations) / len(liked_items) if liked_items else 0

            precisions.append(precision)
            recalls.append(recall)

        return np.mean(precisions), np.mean(recalls)

    def evaluate_diversity(self, recommendations_list):
        """Evaluasi diversity dari rekomendasi"""
        all_brands = set()
        total_recommendations = 0

        for recommendations in recommendations_list:
            if not recommendations.empty:
                brands = set(recommendations['brand'].tolist())
                all_brands.update(brands)
                total_recommendations += len(recommendations)

        diversity = len(all_brands) / total_recommendations if total_recommendations > 0 else 0
        return diversity

# Evaluasi Content-Based Model
cb_evaluator = ContentBasedEvaluator(cb_recommender, final_dataset)

# Sample users untuk evaluasi
sample_users = final_dataset['user_id'].unique()[:10]

# Evaluasi precision dan recall
cb_precision, cb_recall = cb_evaluator.evaluate_precision_recall(sample_users)
cb_f1 = 2 * (cb_precision * cb_recall) / (cb_precision + cb_recall) if (cb_precision + cb_recall) > 0 else 0

print("=== EVALUASI CONTENT-BASED FILTERING ===")
print(f"Precision: {cb_precision:.4f}")
print(f"Recall: {cb_recall:.4f}")
print(f"F1-Score: {cb_f1:.4f}")

# Evaluasi diversity
sample_recommendations = []
for user_id in sample_users:
    user_ratings = final_dataset[final_dataset['user_id'] == user_id]
    if not user_ratings.empty:
        sample_item = user_ratings.iloc[0]['cellphone_id']
        rec = cb_recommender.get_recommendations(sample_item, top_n=5)
        sample_recommendations.append(rec)

cb_diversity = cb_evaluator.evaluate_diversity(sample_recommendations)
print(f"Diversity Score: {cb_diversity:.4f}")

"""Hasil evaluasi metode content-based filtering menunjukkan nilai precision sebesar 0.2200, recall 0.1876, dan F1-score 0.2025. Ini menandakan bahwa model belum terlalu akurat dalam merekomendasikan item yang benar-benar relevan bagi pengguna. Selain itu, diversity score sebesar 0.1200 menunjukkan bahwa variasi item yang direkomendasikan masih rendah, artinya model cenderung memberikan rekomendasi yang mirip-mirip. Secara keseluruhan, performa model masih perlu ditingkatkan agar hasil rekomendasi lebih relevan dan bervariasi.

### Modeling Collaborative Filtering
"""

# Neural Network Collaborative Filtering Model
class NeuralCollaborativeFiltering:
    def __init__(self, num_users, num_items, embedding_size=50, hidden_units=[128, 64]):
        self.num_users = num_users
        self.num_items = num_items
        self.embedding_size = embedding_size
        self.hidden_units = hidden_units
        self.model = None

    def build_model(self):
        """Membangun model neural collaborative filtering"""
        # Input layers
        user_input = layers.Input(shape=(), name='user_id')
        item_input = layers.Input(shape=(), name='item_id')

        # Embedding layers
        user_embedding = layers.Embedding(self.num_users, self.embedding_size, name='user_embedding')(user_input)
        item_embedding = layers.Embedding(self.num_items, self.embedding_size, name='item_embedding')(item_input)

        # Flatten embeddings
        user_vec = layers.Flatten()(user_embedding)
        item_vec = layers.Flatten()(item_embedding)

        # Concatenate user and item vectors
        concat = layers.Concatenate()([user_vec, item_vec])

        # Hidden layers
        x = concat
        for units in self.hidden_units:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.Dropout(0.2)(x)

        # Output layer
        output = layers.Dense(1, activation='linear', name='rating')(x)

        # Create model
        self.model = models.Model(inputs=[user_input, item_input], outputs=output)

        # Compile model
        self.model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return self.model

    def train(self, train_users, train_items, train_ratings,
              validation_data=None, epochs=50, batch_size=256):
        """Melatih model"""
        if self.model is None:
            self.build_model()

        # Early stopping callback
        early_stopping = EarlyStopping(
            monitor='val_loss' if validation_data else 'loss',
            patience=5,
            restore_best_weights=True
        )

        # Training
        history = self.model.fit(
            [train_users, train_items], train_ratings,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping],
            verbose=1
        )

        return history

    def predict(self, users, items):
        """Prediksi rating"""
        return self.model.predict([users, items])

    def get_recommendations(self, user_id, item_ids, top_k=10):
        """Mendapatkan rekomendasi untuk user tertentu"""
        user_array = np.array([user_id] * len(item_ids))
        item_array = np.array(item_ids)

        predictions = self.predict(user_array, item_array)

        # Sort by predicted rating
        item_predictions = list(zip(item_ids, predictions.flatten()))
        item_predictions.sort(key=lambda x: x[1], reverse=True)

        return item_predictions[:top_k]

# Inisialisasi dan training model
ncf_model = NeuralCollaborativeFiltering(
    num_users=len(user_to_idx),
    num_items=len(phone_to_idx),
    embedding_size=50,
    hidden_units=[128, 64, 32]
)

print("=== TRAINING NEURAL COLLABORATIVE FILTERING ===")
print(f"Model architecture:")
ncf_model.build_model()
ncf_model.model.summary()

# Training model
validation_data = ([test_users, test_phones], test_ratings)

history = ncf_model.train(
    train_users, train_phones, train_ratings,
    validation_data=validation_data,
    epochs=30,
    batch_size=128
)

print("âœ“ Model collaborative filtering berhasil dilatih")

"""Model Neural Collaborative Filtering berhasil dilatih dengan arsitektur embedding untuk user dan item, diikuti beberapa layer dense dan dropout. Selama 30 epoch, model menunjukkan penurunan signifikan pada loss dan MAE, dari awalnya di atas 50 menjadi sekitar 4 pada data validasi. Ini menunjukkan bahwa model mampu belajar dengan baik dan memberikan prediksi rating yang semakin akurat seiring waktu.

## Evaluasi

### Evaluasi Content-Based Filtering
"""

# Evaluasi Coverage dan Catalog Coverage
def evaluate_coverage(recommender, all_items, test_users, top_k=10):
    """Evaluasi coverage dan catalog coverage"""
    recommended_items = set()
    total_recommendations = 0

    for user_id in test_users[:20]:  # Sample users
        user_ratings = final_dataset[final_dataset['user_id'] == user_id]
        if not user_ratings.empty:
            sample_item = user_ratings.iloc[0]['cellphone_id']
            recommendations = recommender.get_recommendations(sample_item, top_n=top_k)

            if not recommendations.empty:
                rec_items = set(recommendations['cellphone_id'].tolist())
                recommended_items.update(rec_items)
                total_recommendations += len(rec_items)

    catalog_coverage = len(recommended_items) / len(all_items)
    return catalog_coverage, len(recommended_items)

# Evaluasi Content-Based
all_items = final_dataset['cellphone_id'].unique()
test_user_sample = final_dataset['user_id'].unique()[:20]

cb_coverage, cb_unique_items = evaluate_coverage(cb_recommender, all_items, test_user_sample)

print("=== EVALUASI DETAIL CONTENT-BASED FILTERING ===")
print(f"Catalog Coverage: {cb_coverage:.4f}")
print(f"Unique Items Recommended: {cb_unique_items}")
print(f"Total Items in Catalog: {len(all_items)}")

# Evaluasi Similarity Distribution
def analyze_similarity_distribution(cosine_sim_matrix):
    """Analisis distribusi similarity"""
    # Ambil upper triangle matrix (excluding diagonal)
    upper_triangle_indices = np.triu_indices_from(cosine_sim_matrix, k=1)
    similarities = cosine_sim_matrix[upper_triangle_indices]

    return {
        'mean': np.mean(similarities),
        'std': np.std(similarities),
        'min': np.min(similarities),
        'max': np.max(similarities),
        'median': np.median(similarities)
    }

sim_stats = analyze_similarity_distribution(cosine_sim_matrix)
print(f"\nSimilarity Statistics:")
for key, value in sim_stats.items():
    print(f"{key.capitalize()}: {value:.4f}")

"""Evaluasi detail content-based filtering menunjukkan catalog coverage sebesar 0.8182, artinya model berhasil merekomendasikan 27 dari 33 item yang tersedia, menunjukkan cakupan rekomendasi yang luas. Rata-rata kemiripan antar item yang direkomendasikan adalah 0.2041, dengan median 0.1352 dan standar deviasi 0.3031, menunjukkan variasi yang cukup besar. Nilai kemiripan minimum 0.0 dan maksimum 1.0 mengindikasikan bahwa ada kombinasi item yang sangat tidak mirip maupun sangat mirip dalam hasil rekomendasi.

## Evaluasi Collaborative Filtering
"""

# Evaluasi NCF Model
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Prediksi pada test set
test_predictions = ncf_model.predict(test_users, test_phones)

# Hitung metrik evaluasi
ncf_mse = mean_squared_error(test_ratings, test_predictions)
ncf_mae = mean_absolute_error(test_ratings, test_predictions)
ncf_rmse = np.sqrt(ncf_mse)

print("=== EVALUASI COLLABORATIVE FILTERING ===")
print(f"RMSE: {ncf_rmse:.4f}")
print(f"MAE: {ncf_mae:.4f}")
print(f"MSE: {ncf_mse:.4f}")

# Evaluasi Precision@K dan Recall@K
def evaluate_precision_recall_at_k(model, test_data, user_to_idx, phone_to_idx,
                                   idx_to_user, idx_to_phone, k=5, threshold=7):
    """Evaluasi Precision@K dan Recall@K"""
    precisions = []
    recalls = []

    # Group test data by user
    test_df = pd.DataFrame({
        'user_id': [idx_to_user[u] for u in test_data[0]],
        'cellphone_id': [idx_to_phone[p] for p in test_data[1]],
        'rating': test_data[2]
    })

    for user_id in test_df['user_id'].unique()[:10]:  # Sample users
        user_test_data = test_df[test_df['user_id'] == user_id]

        # Get ground truth (highly rated items)
        relevant_items = user_test_data[user_test_data['rating'] >= threshold]['cellphone_id'].tolist()

        if not relevant_items:
            continue

        # Get all items for recommendation
        all_items = list(phone_to_idx.keys())
        user_idx = user_to_idx[user_id]

        # Get top-k recommendations
        recommendations = model.get_recommendations(user_idx,
                                                   [phone_to_idx[item] for item in all_items],
                                                   top_k=k)

        recommended_items = [idx_to_phone[item_idx] for item_idx, _ in recommendations]

        # Calculate precision and recall
        relevant_recommendations = set(recommended_items) & set(relevant_items)

        precision = len(relevant_recommendations) / len(recommended_items) if recommended_items else 0
        recall = len(relevant_recommendations) / len(relevant_items) if relevant_items else 0

        precisions.append(precision)
        recalls.append(recall)

    return np.mean(precisions), np.mean(recalls)

# Evaluasi Precision@K dan Recall@K
test_data_tuple = (test_users, test_phones, test_ratings)
ncf_precision, ncf_recall = evaluate_precision_recall_at_k(
    ncf_model, test_data_tuple, user_to_idx, phone_to_idx,
    idx_to_user, idx_to_phone, k=5
)

ncf_f1 = 2 * (ncf_precision * ncf_recall) / (ncf_precision + ncf_recall) if (ncf_precision + ncf_recall) > 0 else 0

print(f"\nPrecision@5: {ncf_precision:.4f}")
print(f"Recall@5: {ncf_recall:.4f}")
print(f"F1-Score@5: {ncf_f1:.4f}")

# Plot training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.tight_layout()
plt.show()

"""Evaluasi collaborative filtering menunjukkan performa cukup baik dengan nilai RMSE sebesar 2.0059, MAE 1.4614, dan MSE 4.0237. Grafik training menunjukkan penurunan loss dan MAE yang stabil, mengindikasikan model belajar dengan baik. Namun, performa metrik ranking masih rendah dengan Precision\@5 sebesar 0.0571, Recall\@5 sebesar 0.0952, dan F1-Score\@5 sebesar 0.0714, menandakan bahwa kualitas rekomendasi top-5 masih perlu ditingkatkan.

### Perbandingan Hasil Evaluasi
"""

# Summary evaluasi
print("=== RINGKASAN EVALUASI KEDUA MODEL ===")
print("\nContent-Based Filtering:")
print(f"  - Precision: {cb_precision:.4f}")
print(f"  - Recall: {cb_recall:.4f}")
print(f"  - F1-Score: {cb_f1:.4f}")
print(f"  - Diversity: {cb_diversity:.4f}")
print(f"  - Coverage: {cb_coverage:.4f}")

print("\nCollaborative Filtering:")
print(f"  - RMSE: {ncf_rmse:.4f}")
print(f"  - MAE: {ncf_mae:.4f}")
print(f"  - Precision@5: {ncf_precision:.4f}")
print(f"  - Recall@5: {ncf_recall:.4f}")
print(f"  - F1-Score@5: {ncf_f1:.4f}")

# Visualisasi perbandingan
comparison_metrics = {
    'Model': ['Content-Based', 'Collaborative'],
    'Precision': [cb_precision, ncf_precision],
    'Recall': [cb_recall, ncf_recall],
    'F1-Score': [cb_f1, ncf_f1]
}

comparison_df = pd.DataFrame(comparison_metrics)

plt.figure(figsize=(10, 6))
x = np.arange(len(comparison_df['Model']))
width = 0.25

plt.bar(x - width, comparison_df['Precision'], width, label='Precision', alpha=0.8)
plt.bar(x, comparison_df['Recall'], width, label='Recall', alpha=0.8)
plt.bar(x + width, comparison_df['F1-Score'], width, label='F1-Score', alpha=0.8)

plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Perbandingan Performa Model Rekomendasi')
plt.xticks(x, comparison_df['Model'])
plt.legend()
plt.ylim(0, max(max(comparison_df['Precision']), max(comparison_df['Recall']), max(comparison_df['F1-Score'])) + 0.1)

for i, model in enumerate(comparison_df['Model']):
    plt.text(i-width, comparison_df['Precision'][i] + 0.01, f'{comparison_df["Precision"][i]:.3f}', ha='center')
    plt.text(i, comparison_df['Recall'][i] + 0.01, f'{comparison_df["Recall"][i]:.3f}', ha='center')
    plt.text(i+width, comparison_df['F1-Score'][i] + 0.01, f'{comparison_df["F1-Score"][i]:.3f}', ha='center')

plt.tight_layout()
plt.show()

"""Model Content-Based Filtering memiliki performa lebih baik dibandingkan Collaborative Filtering, dengan nilai precision, recall, F1-score, dan coverage yang lebih tinggi. Oleh karena itu, Content-Based lebih efektif untuk sistem rekomendasi dalam kasus ini."""